Formal methods in ML - Shared Doc

Chat from meeting on 10.01.2022 (To read):

Sara Mohammadinejad, Brandon Paulsen, Jyotirmoy V. Deshmukh, Chao Wang: DiffRNN: Differential Verification of Recurrent Neural Networks. FORMATS 2021: 117-134 

Anand Balakrishnan, Jyotirmoy Deshmukh, Bardh Hoxha, Tomoya Yamaguchi, Georgios Fainekos: PerceMon: Online Monitoring for Perception Systems. RV 2021: 297-308 

Anand Balakrishnan, Aniruddh Gopinath Puranic, Xin Qin, Adel Dokhanchi, Jyotirmoy V. Deshmukh, Heni Ben Amor, Georgios Fainekos: Specifying and Evaluating Quality Metrics for Vision-based Perception Systems. DATE 2019: 1433-1438 

Luan Viet Nguyen, James Kapinski, Xiaoqing Jin, Jyotirmoy V. Deshmukh, Taylor T. Johnson: Hyperproperties of real-valued signals. MEMOCODE 2017: 104-113 
               https://dblp.org/pid/96/11505.html 

Automated Safety Verification of Programs Invoking Neural Networks 

Cumhur Erkan Tuncali, Georgios Fainekos, Danil V. Prokhorov, Hisahiro Ito, James Kapinski: Requirements-Driven Test Generation for Autonomous Vehicles With Machine Learning Components. IEEE Trans. Intel I. Veh. 5(2): 265-280 (2020) 

Mohammad Hekmatnejad, Bardh Hoxha, Georgios Fainekos: Search-based Test-Case Generation by Monitoring Responsibility Safety Rules. CoRR abs/2005.00326 (2020)








Authors
Purpose
Brief description
Type of NN
Underlying formalism
Type of solver used 
1
Introduction to Neural Network Verification (Book)
	
Aws Albarghouthi 
It serves as an introduction to NNs and their verification 
It describes representation of NNs as data-flow graphs. It then talks about constraint-based and abstraction-based techniques for NNs
Feed-forward DNNs
-
-
2
ReLUPLEX
Guy Katz et al
Framework for verifying DNNs involving ReLUs. 
The simplex algorithm for solving LP instances is extended to support ReLU constraints by incrementally satisfying constraints that they impose
DNNs with ReLUs - 8 layers, 300 ReLU nodes
ReLU constraints represented as linear programs, and solved incrementally by using bound tightening on variables involved 
SMT solver
3
DiffRNN
Jyotirmoy Deshmukh et al
To certify equivalence of two structurally similar neural networks
DiffRNN pairs neurons and edges of first network with second, and directly computes difference intervals layer-by-layer.
Vanilla RNN, LSTM
Bounding nonlinear activation functions with linear constraints, solving constrained optimization problems to compute tight bounding boxes on non-linear surfaces ( 
Interval analysis)
dReal SMT solver used to prove soundness of bounding boxes
4
NeuroSPF
Muhammad Usman et al
Symbolic analysis of NNs 
It builds on Symbolic Path Finder (SPF) for Java programs. Therefore, it translates NN to Java and then runs SPF on it.
Feed-forward DNN specified in Keras
Symbolic analysis, execution
-
5
Automated Safety Verification of programs invoking NNs
Maria Christakis et al
Verifying safety of a heterogeneous systems that use NNs
Two-way integration of C-program and NN analysis.
-
Abstract interpretation
-
6
Safety verification of deep neural networks
Xiaowei Huang et al
Automated verification of NNs based on SMT
Adversarial perturbations targeted
It checks for point-wise robustness by exhaustively searching the region (within a specified diameter) around point(s), where all the points in the region are supposed to be in the same class. It verifies layer-by-layer, until either a misclassification is found or it is guaranteed that no misclassification is present. 
Feed-forward, multilayer NN
Search based recursive verification.

Targets point-wise robustness
Z3
		


7
Simulation-based Adversarial Test Generation for Autonomous Vehicles with Machine Learning Components 
Cumhur Erkan Tancali (Toyota Research)
Simulation based test


Autonomous system with DNN
i.Global Uniform Random Search, 

ii.Covering arrays and uniform random search, 

iii.Covering arrays and simulated annealing
-
8
Search-based Test-Case Generation by Monitoring Responsibility Safety Rules
	Mohammad Hekmatnejad 
Using monitoring and falsification to generate qualified test cases
Responsibility Sensitive Safety (RSS) rules are used as qualifier specifications to filter out the random tests that do not satisfy the RSS assumption. The remaining tests cover driving scenarios that the controlled vehicle does not respond safely to its environment.
Simulated driving scenarios
A requirement guided search on simulated driving scenarios.





9
DeepConcolic
Xiaowei Huang et al
It uses concolic testing (i.e. concrete execution + symbolic analysis) to explore execution paths.
Start with concrete execution of a random test input, t. If t is close to satisfying a requirement, apply symbolic analysis to obtain a new test input t’ that satisfies the requirement. Iterate until a satisfactory level of coverage achieved. 
Feed-forward DNN
Concolic testing
Coverage criteria: Lipschitz coverage(~structural coverage), MC/DC, neuron boundary coverage, neuron coverage (~ statement coverage)
(Fragment of Quantified Linear Arithmetic over rationals used to express coverage requirements)
The formula resulting from the symbolically encoded execution path is solved by a constraint solver.
TensorFuzz
Coverage guided fuzzing for DNNs












*DNNV
David Shriver et al
A framework to facilitate the application, development, and comparison of DNN verifiers 
DNNV standardizes input and output formats, property specification (using DSL), provides simplification and reduction operations to facilitate the application, development, and comparison of DNN verifiers. This also helps increase the support of ML verifiers for existing benchmarks.
*Hyperproperties of Real-Valued Signals 
Jyotirmoy Deshmukh
A testing technique that allows to check or falsify hyperproperties of CPS models.  
				
			
		


STL is extended to HyperSTL in order to represent hyperproperties.
They specify and falsify HyperSTL properties for two case studies involving automotive control systems (with no ML components)

________________________________________________________________________________________________________________

                            Gray-box monitoring of hyperproperties
				





Runtime verification (RV) is concerned with 
(1) generating a monitor from a formal specification φ, and 
(2) using the monitor to detect whether or not φ holds by observing events generated by the system at run time. 

Monitorability - possibility of monitoring a property. Some properties non-monitorable because no finite observation can lead to a conclusive verdict.
A formula φ is (semantically) monitorable if every observation O has an extended observation P ≽ O, such that P |=s φ or P |=v φ.			s- satisfies; v- violates

∀π.∀π.(aπ ↔ ¬aπ′ ) is monitorable, but ∀π.∃π.(aπ ↔ ¬aπ′ ) is not, as it requires an observation set of infinite size

A formula φ is seman-tically gray-box monitorable for a system S if every observation O has an extended observation P ≽O in S, such that P |=sS φ or P |=vS φ.

Given a property φ and a set of behaviors S, a monitor M is sound whenever, for every observation O ∈ O,
1. ifO|=sS φ,thenM(O)=⊤orM(O)=?, 
2. ifO|=vS φ,thenM(O)=⊥orM(O)=?, 
3. otherwise M(O) = ?

Given a property φ and a set of traces S, a monitor M is perfect whenever, for every observation O ∈ O,
1. ifO|=sS φthenM(O)=⊤, 
2. ifO|=vS φthenM(O)=⊥, 
3. otherwise M(O) = ?.

A property φ is strongly monitorable for a system S if there is a sound monitor M s.t. for all observations O ∈ O, there is an extended observation P ∈ S for which either M(P) = ⊤ or M(P) = ⊥

if a property is not semantically monitorable, then it is not strongly monitorable but in rich domains, some semantically monitorable properties may not be strongly monitorable

If φ is strongly monitorable, then φ is semantically monitorable.
strongly monitorable => semantically monitorable
!semantically monitorable => !semantically monitorable

we propose to use a combination of static analysis and runtime verification to monitor violations of ∀+∃+ properties (or dually, satisfactions of ∃+∀+). The main idea is to collect candidates for the outer ∃ part dynamically and use static analysis at runtime to over-approximate the inner ∀ quantifiers.


(P1) DDM is not semantically black-box monitorable,
(P2) DDM is semantically white-box monitorable (for programs that are not DDM),
(P3) checking DDM statically is undecidable,
(P4) DDM is strongly gray-box monitorable for violation, and we give a sound monitor.

The static analyzer receives the finite observation O collected by the monitor, but not the future system behavior. Instead it must reason under the assumption that any system behavior in S that is compatible with O, may eventually occur.For example, given an ∃∀ formula, the outer existential quantifier is instantiated with a concrete set U of runtime traces, while possible extensions of U provided by static analysis can be used to instantiate the inner universal quantifier.

Given a function f : I → O, the problem of data minimization consists in finding a preprocessor function p: I → I, such that f = f ◦p and p = p◦p. The goal of p is to limit the information available to f while preserving the behavior of f

For f : I → O with finite I , φdm is strongly monitorable in Sf . If I is infinite, then φdm is not semantically monitorable for satisfaction, but we can still hope to build a sound monitor for violation of φdm.


________________________________________________________________________________________________________________

Explaining Image Classifiers using Statistical Fault Localization (Explainable AI)

a black-box explanation technique based on statistical fault localization.

static fault localisation based on local perturbations

a good explanation gives an answer to the question
“why did this outcome occur

 An explanation in image classification is a minimal subset of pixels of a given input image that is sufficient for the DNN to classify the image, where “sufficient” is defined as containing only this subset of pixels from the original image, with the other pixels set to the background colour.


"cowboy hat”image. Although Xception labels the input image correctly, an explanation produced by DeepCover indicates that this decision is not based on the correct feature (the hat in the image), but on the face, which is an unexpected feature for the label ‘cowboy hat’. While this image was not, technically speaking, misclassified, the explanation points to a flaw in the DNN’s reasoning. 
The explanations generated by DeepCover may thus be useful for assessing the adequacy of the DNN training: they allow us to check, whether the DNN is aligned with the developer’s intent during training.
a DNN is “trojaned” if it behaves correctly on ordinary input images but exhibits malicious behavior when a “Trojan trigger” is part of the input.


________________________________________________________________________________________________________________

Seshia, Sanjit A., et al. "Formal specification for deep neural networks." International Symposium on Automated Technology for Verification and Analysis. Springer, Cham, 2018.


Global robustness:
1)
x: fixed input       𝛿: perturbation        β: fixed bound      
μ: cost function defined on the perturbations, typically a distance metric based on a norm (L1,L2,or L∞)

∀x. ∀δ. ¬φ(δ)
 where ¬φ(δ) is : [μ(δ) < β ∧ δ ∈ Δ] ⇒ [fw(x + δ) ∉ T(x)]

constraint fw (x + δ) ∉ T (x) ensures that the output of the NN to the perturbed input does not lie in the adversary’s target output set T(x) ((which can be a function of x, e.g., Y \ {y} where y is the correct label))

2)
∀x1,x2. [μ(x1 − x2) < β ∧ (x1 −x2) ∈ Δ]⇒ [fw(x1) ≈ fw(x2)]

DNN outputs a similar answer on all pairs of inputs (x1,x2) that are “close”; where “≈” is a suitably-defined notion of similarity between outputs of the DNN.


Ruan, Wenjie, et al. "Global robustness evaluation of deep neural networks with provable guarantees for the hamming distance." IJCAI, 2019.
-global robustness - defined as the expected maximum safe radius (using Hamming distance) over a (finite) test dataset, which is a generalisation of the local, pointwise robustness problem 
-generate sequences of lower and upper bounds for global robustness
-global robustness aims to capture the worst case local robustness (worst case maximal safety radius)
-quantifies the size of the perturbation that the system can withstand for a set of inputs, instead of just a single input
-bounds are computed by iteratively applying perturbations for every input until a misclassification occurs. GPU parallelisation used.


Fairness: 
Albarghouthi, Aws, et al. "Fairness as a program property." arXiv preprint arXiv:1610.06067 (2016).

- defined for decision-making systems
- output should not be influenced by certain sensitive attributes of input

specify pre-condition on inputs say age ~ gauss(18,5) // the age has a gaussian distribution

and a post-condition:

i) Individual/Similarity-based fairness: Similar inputs mapped to similar outputs
    Pr [hire(v) ≠ hire(v') | v ~ v'] < ϵ

ii) Group fairness: Probability of getting a particular output is independent of the values of sensitive input attributes (similar to non-interference)

    e.g. the algorithm is just as likely to hire a minority applicant as a non-minority applicant
    
    Pr[hire(v) = true | v = minority]
    -------------------------------    > 1 - ϵ
    Pr[hire(v) = true | v ≠ minority]    

    An output value of true implies hired.

proving program fair boils down to statically checking whether on inputs satisfying pre-condition, outcome      satisfies post-condition
 

 Friedler, Sorelle A., Carlos Scheidegger, and Suresh Venkatasubramanian. "On the (im) possibility of fairness." arXiv preprint arXiv:1609.07236 (2016).

 Fix two thresholds ϵ, ϵ'. Then f is (ϵ, ϵ')-fair if for any x,y ϵ P
    distance (x,y) ≤ ϵ ⇒ distance (f(x), f(y)) ≤ ϵ'

 Group fairness: Pr[hire = yes | majority = 1] - Pr[hire = yes | majority = 0]
                This is called the discrimination score and the goal is to bring it close to zero.

________________________________________________________________________________________________________________

Gray-box monitoring of hyper-properties 
- hyper-property: Distributed data minimization

combination of static analysis and runtime verification used to monitor violations of ∀+∃+ properties
-collect candidates for ∃ part dynamically and use static analysis at runtime to over-approximate the ∀ quantifiers

(∀ is replaced by conjunctions over finite set of input traces; ∃ replaced by a single quantifier ranging over all input traces)
extract predicate (say, φ) from system under observation using symbolic execution  -> extend it to FOL formulas over sets of observed traces ( φ(x,y) holds?) -> check the result using Z3

________________________________________________________________________________________________________________

Explainable AI - 
Sun, Youcheng, et al. "Explaining image classifiers using statistical fault localization." European Conference on Computer Vision. Springer, Cham, 2020.

-Use static fault localisation to rank input features (pixels)
-Explanation in image classification is the minimal subset of pixels of input image that is sufficient for DNN to classify the image

-Given an input image x classified by DNN as y, generate random mutations of x (by masking pixels which are not of interest with background colour)
-Annotate them with y or ~y depending on the output generated by DNN
-Rank the pixels 
-Construct an explanation by iteratively adding pixels in descending prder of rank until set becomes sufficient for DNN to classify the image

Other techniques:
- finding contribution scores of neurons to final output
-gradient-based algorithms need one backward pass
-sample neighbourhood of the input and create a linear model to approximate model's local behaviour
-measure flow of information between inputs and outputs to estimate importance of input features

________________________________________________________________________________________________________________

To do:

1. Put input into cost function in definition 1
    ∀x. ∀δ. ¬φ(δ)
 where ¬φ(δ) is : [μ(δ) < β ∧ δ ∈ Δ] ⇒ [fw(x + δ) ∉ T(x)]

2. Relation between confidence of NN amd perturbation/input

3. Put input into cost function in definition 2
    ∀x1,x2. [μ(x1 − x2) < β ∧ (x1 −x2) ∈ Δ]⇒ [fw(x1) ≈ fw(x2)]

Xu, Huan, and Shie Mannor. "Robustness and generalization." Machine learning 86.3 (2012): 391-423.

   - If testing sample close to training sample, then testing error close to training error. 
   - Partition the sample set into finite subsets s.t. if a new sample falls into the same subset as a training     sample, then the loss of former is close to loss of latter. 
 (loss -> generalization errornof learning algorithm)

Wu, Xi, et al. "Reinforcing adversarial robustness using model confidence induced by adversarial training." International conference on machine learning. PMLR, 2018. 

- combines confidence information and nearest neighbour search to embed low confidence point back to high confidence regions

    adversary draws x(input) ->  produces x' = x + Δ  ->  sends x' to defender  ->  defender outputs label l
     if C(x) = C(x'), defense succeeds
    considers nearby points (specified by allowable perturbations)

    try to find a point around x which has high confidence prediction and is near to x
    arg min |z - x|   s.t z ∈ N(x,ξ)
      subject to ||F (z)||∞ ≥ p
    N(x,ξ) is the ξ-ball around x w.r.t. ||.||


∀x. ∀δ. ¬φ(δ)
 where ¬φ(δ) is : [μ(δ) < β ∧ δ ∈ Δ] ⇒ [fw(x + δ) ∉ T(x)]

conf(f(x)): confidence of NN on input x

Robustness:
∀ x, x’, ε>0. ∃ δ>0. |x-x’| <= δ and ⇒ |f(x)-f(x’)| <= ε

Robustness + confidence:
∀ x, x’, ε>0. ∃ δ>0. |x-x’| <= δ and conf(f(x)) > k ⇒ |f(x)-f(x’)| <= ε

∀ x, x’, ε>0. ∃ δ>0. |x-x’| <= δ ⇒ w(conf(f(x)))*|f(x)-f(x’)| <= ε

X - set of inputs
L - set of output labels

f : X -> [1,0]^|L|
For all x, f(x) = (c1; …, c_|L|) implies that c1 + .. + c_|L| = 1 and c_i >= 0 

∀ x, x’, ε>0. ∃ δ>0. |x-x’| <= δ ⇒ |f(x)-f(x’)| <= ε

_____________________________________________________________________________________________________

1.Robustness and confidence (reachability vs hyperproperty)
2.Parameter synthesis
3.Maria: Look for instantiations that hold with high probability (e.g., above a threshold)?
4.Static analysis of 2-safety in DNNs 
5.Combining 2-safety for systems and DNNs (Maria’s work on reachability lifted to 2-safety)
6.Combine static and dynamic analysis
7.Multi-view specifications (interface theory, conjunction)

_____________________________________________________________________________________________________

- Tensorflow/PyTorch
- Gradient descent/ascent
- Create adversarial example FGSM on a smaill MNIST - Tensorflow
- Optimizer - fine tune NN parasmeters
- ADAM, RMSProp
- activation functions
- train an MNIST NN
- www.deeplearningbook.org - chapters: 6, 11, 12
- deepxplore - neuron coverage paper
_____________________________________________________________________________________________________

robustness (with confidence) definition in DSL:
    pre-condition: 
        {x' = differ (x, 𝛿)}  where c = differ(a, b) means c and a differ by distance b 

    post-condition:
        {conf(f(x)) > k ⇒ f(x') = differ(f(x), ε)}
	
_____________________________________________________________________________________________________


Globally Robust Networks (CMU)
 
-Train certifiably robust neural networks, 
-Use global robustness definition 
    use global Lipschitz bounds to train models that are certifiably robust
-Introduce additional output class for non-trivial behaviour (for points abrbitrarily close to a decision boundary) - 
 signals that a point can't be certified as globally robust: no 2 points at distance ε form each other are labeled 
 with different non-'additional' classes
-deterministic guarantees on l2-bounded perturbations
-instrument a model with an extra output, ⊥, that labels a point as “not locally-robust,” such that the instrumented 
model predicts a non-⊥ class only if the point is locally-robust (with respect to the original model). At a high level, 
ensure that in order to avoid predicting ⊥, the maximum output of f must surpass the other outputs by a sufficient 
margin. (While this margin is measured in the output space, we can ensure it is sufficiently large to ensure local 
robustness by relating the output space to the input space via an upper bound on the model’s Lipschitz constant)

|fi(x1)−fi(x2)|
---------------  ≤ Ki
||x1−x2|| 

-Local bound needed to be computed for every point, global can be computed in advance
-MNIST, CIFAR-10, Tiny-Imagenet
-Train GloRo networks to cetify robustness against l2 perturbations within ε-neighbourhood of 0.3 and 1.58 for 
 MNIST (standard l2 norms used)
-MinMax activations perform better than ReLU for gloro nets, so minmax used 
-outperforms state-of-the-art in deterministic verified-robust accuracy
-performs certification in a single forward pass, so is fast

-Do not include softmax

_____________________________________________________________________________________________________

Learning Security Classifiers with Verified Global Robustness Properties

- a framework and tools for training classifiers that satisfy global robustness properties
- Our algorithm trains a verifiably robust classifier: we can formally verify that F satisfies 𝜑
- To evaluate the local robustness of a trained model, we can measure the percentage of data points
𝑥 in the test set that satisfy 𝜑(𝑥)
-start from a classifier without the global robustness property, use a verifier to find counter-examples that 
violate the property, and train the classifier for one epoch guided by the counterexample.This process is repeated 
until the classifier satisfies the property
-The classifier is structured as an ensemble of logic rules. For example, 
 “wasm < 0.5 ∧ web workers < 1.5 → −1.99" means that if the website does not use WebAssembly, and has at most
 one web worker, the clause adds 𝑅_0 i.e. -1.99 to the final prediction value. Otherwise, the clause is inactive and 
 adds nothing to the final prediction value. 0.5, 1.5 and -1.99 are learnable parameters
 
 Global Robustness Property Definition:
 
 1. Property 1 (Monotonicity): Given a feature 𝑗,
    ∀𝑥,𝑥′∈ R𝑛.[𝑥𝑗 ≤ 𝑥′𝑗 ∧ (∀𝑖≠𝑗. 𝑥𝑖 = 𝑥′𝑖)] => F(𝑥) ≤ F(𝑥′)
    
    This property specifies that the classifier is monotonically increasing along some feature dimension. 
    It is useful to defend against a class of attacks that insert benign features into malicious instances
    If we carefully choose features to be monotonic for a classifier, injecting content into a malicious instance
    can only make it look more malicious to the classifier, i.e., these changes can only increase 
    classification score. 
    
 2. Property 2 (Stability):Given a feature 𝑗 and a constant 𝑐,
    ∀𝑥,𝑥′∈ R𝑛.[∀𝑖≠𝑗.𝑥𝑖 = 𝑥′𝑖] => |F(𝑥)−F(𝑥′)| ≤ 𝑐
    
    for all 𝑥,𝑥′, if they only differ in the 𝑗-th feature, the difference between their prediction scores is
    bounded by a constant (Lipschitz constant) 𝑐 
    
 3. Property 3 (High Confidence):Given a set of low-cost features 𝐽,
    ∀𝑥,𝑥′∈R𝑛.[∀𝑖∉𝐽.𝑥𝑖=𝑥′𝑖]∧𝑔(F(𝑥)) ≥ 𝛿 => F(𝑥′) ≥ 0
    
    The high confidence property states that, for any sample 𝑥 that is classified as malicious with high confidence 
    (e.g.,𝛿=0.98), perturbing any low-cost feature 𝑗 ∈ 𝐽 does not change the classifier prediction from malicious to 
    benign
    
 4. Property 3a (Maximum Score Decrease): Given a set of low-cost features 𝐽,
    ∀𝑥,𝑥′∈ R𝑛.[∀𝑖∉𝐽.𝑥𝑖 = 𝑥′𝑖] => F(𝑥)−F(𝑥′) ≤ g^(−1)(𝛿)
    
    If the maximum decrease of any classification score is bounded by g^(−1)(𝛿), then any high confidence 
    classification score does not drop below zero
    
 5. Property 4 (Redundancy): Given 𝑀 groups of low-cost features 𝐽1, 𝐽2, . . ., 𝐽𝑀
    If the attacker perturbs multiple low-cost features, we would like the high confidence predictions from the 
    classifier to be robust if different groups of low-cost features are not perturbed at the same time. In the 
    redundancy property, we identify 𝑀 groups of low-cost features, and require that the attacker has to perturb at 
    least one feature from each group in order to evade a high confidence prediction
    
 6. Property 5 (Small Neighborhood): Given a constant 𝑐, 
    ∀𝑥,𝑥′∈R𝑛.𝑑(𝑥,𝑥′) ≤ 𝜖 => |F(𝑥)−F(𝑥′)| ≤ 𝑐·𝜖  
    where 𝑑(𝑥,𝑥′)=max𝑖{|𝑥𝑖 − 𝑥′𝑖|/𝜎𝑖}
    
    The small neighborhood property specifies that for any two data points within a small neighborhood defined by 𝑑,
    we want the classifier’s output to be stable. We define the neighborhood by a new distance metric 𝑑(𝑥,𝑥′) that
    measures the largest change to anyfeature value, normalized by the standard deviation of that input feature.
    𝑑(𝑥,𝑥′)is essentially a ℓ∞ norm, applied to normalized feature values. 
    ** We chose not to use the ℓ∞ distance directly because different features for security classifiers often have a 
    different scale
    
    
  - We propose a new type of classification model, which we call a logic ensemble. We show how to train logic ensemble
  classifiers that satisfy global robustness properties
  Logic ensembles can be viewed as a generalization of decision trees
  
  A logic ensemble classifier F consists of a set of clauses. Each clause has the form
  𝐵1(𝛼1,𝛽1) ∧ 𝐵2(𝛼2,𝛽2)∧···∧ 𝐵𝑚(𝛼𝑚,𝛽𝑚) → R
  where 𝐵1. . .𝐵𝑚 are atoms and 𝑅 is the activation value of the clause
  
  Verification algorithm:
  
  - new verification algorithm that uses integer linear programming to verify the global robustness properties of 
  logic ensembles, including trees:
  i. encode the logic ensemble using boolean variables, adding consistency constraints among the boolean variables
  ii. for each global robustness property, we symbolically represent the input and output of the classifier in terms 
  of these boolean variables, and add extra constraints to assert that the robustness property is violated
  iii. check feasibility of these constraints, expressing them as a 0/1 integer linear program
  iv. gives feasible solution -> classifier does not satisfy the global robustness property -> counterexample; else 
  satisfies prop
 
- When the training algorithm terminates, if it finds a classifier, the classifier is guaranteed to satisfy the 
  properties. However, there is no guarantee that it will find a classifier, but empirically can find 
  an accurate classifier that satisfies all the specified properties 
  
  
  Evaluation:
 -Twitter Spam Accounts - collect information about Twitter spam accounts, and randomly sampled benign Twitter users. 
 -We reimplement 15 of their proposed features, including account age, number of following, number of followers, etc 
 -We identify 8 low-cost features in total
 -We specify two features to be monotonically increasing, and two features to be monotonically decreasing, based on domain 
 knowledge
 
 Comparing results:
 We compare against three types of base-line models, 
 (1) tree ensemble and neural network that are not trained using any properties, 
 (2) monotonic classifiers, and 
 (3) neural network models trained with local robustness versions of our properties - lot of counter-exs found,
 NN (DL2) models cannot obtain global robustness
 
 Citation: Fischer et al. and Melacci et al. proposed global robustness properties for image classifiers using 
 universally quantified statements. Both of their techniques smooth the logic expression of the property into a 
 differentiable loss function, and then use PGD attacks to minimize the loss. They can train neural networks to 
 obtain local robustness, but cannot obtain verified global robustness








_____________________________________________________________________________________________________
Beyond Robustness: Resilience Verifcation of Tree-Based Classifiers

-For binary decision trees
-Resilience (generalization of robustness) in place of robustness
-identify a subset of feature space where model predictions not changed despite adversarial manipulations
-* a classifier is called stable  on an instance if for all adversarial manipulations of the instance, makes same
prediction. However stability is trivial - a classifier that always predicts one class is stable! Robustness 
additionally requires that correct prediction performed.
- Limitation of robustness : quantified over a specific test set
-They define robustness as : if the classifier makes correct prediction on the test set
-Consider all instances within L-inf distance from instance x. neighborhood of x thus represented by a small box around
x.
-Resilience requires the classifier to be robust on x while remaining stable on instances in neighbourhood of x, for 
which correct class labels are unknown
-compute with static analysis a sound under-approximation of the portion of feature space where classifier is stable.


Additional benchmarks (similar to ACASXu): https://github.com/sisl/HorizontalCAS
_____________________________________________________________________________________________________

Real NN f(x)
Approximated NN f'(x)
 
Spec: forall x1, x2. |x1-x2| <= epsilon and conf(f(x1)) >= delta -> f(x) == f(x')
 
Theorem: Let gamma be the approximation constant. Then forall x. |conf(f(x)) - conf(f'(x))| < gamma
 
Theorem: Let f and f' such that forall x. |conf(f(x)) - conf(f'(x))| < gamma. Then, we have that
 
forall x1, x2. |x1-x2| <= epsilon and conf(f'(x1)) >= delta + gamma -> f'(x1) == f'(x2)
->
forall x1, x2. |x1-x2| <= epsilon and conf(f(x1)) >= delta -> f(x1) == f(x2)
 
------------------
 
phi == forall x, x'. |x-x'| <= epsilon and conf(f(x)) >= delta -> f(x) == f(x')
phi'(gamma) == forall x, x'. |x-x'| <= epsilon and conf(f'(x)) >= delta + gamma -> f'(x) == f'(x')
 
 
-----------------
 
CEGAR
max_iter = N
iter = 1
 
while(iter <= max_iter)
  f', gamma = approx(f, iter)
  verdict, (ce1, ce2), conf = f' |= phi'(gamma)
 
  if verdict == sat:
    return verdict
  else if verdict == unsat:
    if f(ce1) != f(ce2):
      return verdict, (ce1, ce2)
    else:
      if search((ce1, ce2) s.t. conf(f(ce1) >= delta and f(ce1) != f(ce2)):
                return verdict, (ce1, ce2)
      else   
               iter = iter + 1
 
return unknown

_____________________________________________________________________________________________________

log 2 = 0.6931
log 3 = 1.0986
log 4 = 1.3863
log 5 = 1.6094
log 6 = 1.7917
log 7 = 1.9459
log 8 = 2.0794
log 9 = 2.1972
log 10 = 2.3025
_____________________________________________________________________________________________________

Adult datset description - https://cseweb.ucsd.edu/classes/sp15/cse190-c/reports/sp15/048.pdf



















